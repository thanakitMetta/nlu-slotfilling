{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install seqeval\n!pip install sklearn_crfsuite\n!pip install git+https://www.github.com/keras-team/keras-contrib.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-20T15:17:16.593341Z","iopub.execute_input":"2023-04-20T15:17:16.593803Z","iopub.status.idle":"2023-04-20T15:18:00.150956Z","shell.execute_reply.started":"2023-04-20T15:17:16.593748Z","shell.execute_reply":"2023-04-20T15:18:00.149588Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.21.6)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.0.2)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=9cc318ebb38eea7d12e0b36ec31facd77ee9b8839eba930f57d31c8fa2b4b282\n  Stored in directory: /root/.cache/pip/wheels/b2/a1/b7/0d3b008d0c77cd57332d724b92cf7650b4185b493dc785f00a\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting sklearn_crfsuite\n  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.7/site-packages (from sklearn_crfsuite) (4.64.1)\nCollecting python-crfsuite>=0.8.3\n  Downloading python_crfsuite-0.9.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (966 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m966.9/966.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sklearn_crfsuite) (1.16.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from sklearn_crfsuite) (0.9.0)\nInstalling collected packages: python-crfsuite, sklearn_crfsuite\nSuccessfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://www.github.com/keras-team/keras-contrib.git\n  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-24awcyap\n  Running command git clone --filter=blob:none --quiet https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-24awcyap\n  warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n  Resolved https://www.github.com/keras-team/keras-contrib.git to commit 3fc5ef709e061416f4bc8a92ca3750c824b5d2b0\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (from keras-contrib==2.0.8) (2.11.0)\nBuilding wheels for collected packages: keras-contrib\n  Building wheel for keras-contrib (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101076 sha256=f0362dafd26ce0125bfa877edccfcb1ff885be13f7477a5898a0de7e0e01a677\n  Stored in directory: /tmp/pip-ephem-wheel-cache-uav9lxbe/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\nSuccessfully built keras-contrib\nInstalling collected packages: keras-contrib\nSuccessfully installed keras-contrib-2.0.8\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"pip install prettytable","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:18:00.154035Z","iopub.execute_input":"2023-04-20T15:18:00.154434Z","iopub.status.idle":"2023-04-20T15:18:09.968739Z","shell.execute_reply.started":"2023-04-20T15:18:00.154390Z","shell.execute_reply":"2023-04-20T15:18:09.967359Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: prettytable in /opt/conda/lib/python3.7/site-packages (0.7.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install nltk","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:18:09.971538Z","iopub.execute_input":"2023-04-20T15:18:09.971999Z","iopub.status.idle":"2023-04-20T15:18:20.197536Z","shell.execute_reply.started":"2023-04-20T15:18:09.971946Z","shell.execute_reply":"2023-04-20T15:18:20.195981Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV, PredefinedSplit\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom prettytable import PrettyTable\nimport nltk\nfrom nltk.corpus import stopwords\nimport sklearn_crfsuite\nfrom sklearn_crfsuite import metrics\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import classification_report\nfrom itertools import chain\nimport shutil\nfrom keras_contrib.layers import CRF\nfrom keras_contrib.losses import crf_loss\nfrom keras_contrib.metrics import crf_accuracy\nimport pickle\nimport random\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:18:20.201877Z","iopub.execute_input":"2023-04-20T15:18:20.202252Z","iopub.status.idle":"2023-04-20T15:18:35.847789Z","shell.execute_reply.started":"2023-04-20T15:18:20.202216Z","shell.execute_reply":"2023-04-20T15:18:35.846593Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"nltk.download('all')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:18:35.849449Z","iopub.execute_input":"2023-04-20T15:18:35.851066Z","iopub.status.idle":"2023-04-20T15:18:59.555136Z","shell.execute_reply.started":"2023-04-20T15:18:35.851026Z","shell.execute_reply":"2023-04-20T15:18:59.552457Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package bcp47 to /usr/share/nltk_data...\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n[nltk_data]    | Downloading package comtrans to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/dolch.zip.\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package extended_omw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package floresta to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n[nltk_data]    | Downloading package nps_chat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package paradigms to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pe08 to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/pe08.zip.\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/perluniprops.zip.\n[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package porter_test to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package ppattach to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package propbank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package state_union to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package tagsets to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package verbnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n[nltk_data]    | Downloading package webtext to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet2022 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import json","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:18:59.557484Z","iopub.execute_input":"2023-04-20T15:18:59.558688Z","iopub.status.idle":"2023-04-20T15:18:59.571576Z","shell.execute_reply.started":"2023-04-20T15:18:59.558642Z","shell.execute_reply":"2023-04-20T15:18:59.563854Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Read data from JSON file\nwith open('/kaggle/input/hackathon-online-nlu-slot-filling2/train.json') as json_file: \n    data_dict = json.load(json_file) \n    \nwith open('/kaggle/input/hackathon-online-nlu-slot-filling2/valid.json') as json_file: \n    vdata_dict = json.load(json_file)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:18:59.575748Z","iopub.execute_input":"2023-04-20T15:18:59.576680Z","iopub.status.idle":"2023-04-20T15:19:02.342427Z","shell.execute_reply.started":"2023-04-20T15:18:59.576630Z","shell.execute_reply":"2023-04-20T15:19:02.340903Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_utterance = []\ntrain_slots = []\n\nval_utterance = []\nval_slots = []\n# Load data into respective dicts\nfor data in data_dict:\n    train_utterance.append(data['utterance'])\n    train_slots.append(data['slots'])\n    \nfor vdata in vdata_dict:\n    val_utterance.append(vdata['utterance'])\n    val_slots.append(vdata['slots'])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:02.344544Z","iopub.execute_input":"2023-04-20T15:19:02.345021Z","iopub.status.idle":"2023-04-20T15:19:02.369429Z","shell.execute_reply.started":"2023-04-20T15:19:02.344961Z","shell.execute_reply":"2023-04-20T15:19:02.367939Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#train_utterance","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:02.372982Z","iopub.execute_input":"2023-04-20T15:19:02.374316Z","iopub.status.idle":"2023-04-20T15:19:02.453261Z","shell.execute_reply.started":"2023-04-20T15:19:02.374268Z","shell.execute_reply":"2023-04-20T15:19:02.451943Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#train_slots","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:02.459058Z","iopub.execute_input":"2023-04-20T15:19:02.459698Z","iopub.status.idle":"2023-04-20T15:19:02.550300Z","shell.execute_reply.started":"2023-04-20T15:19:02.459666Z","shell.execute_reply":"2023-04-20T15:19:02.549049Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(train_utterance),len(train_slots)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:02.551719Z","iopub.execute_input":"2023-04-20T15:19:02.552332Z","iopub.status.idle":"2023-04-20T15:19:02.635320Z","shell.execute_reply.started":"2023-04-20T15:19:02.552292Z","shell.execute_reply":"2023-04-20T15:19:02.633743Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(13084, 13084)"},"metadata":{}}]},{"cell_type":"code","source":"len(val_utterance),len(val_slots)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:02.636558Z","iopub.execute_input":"2023-04-20T15:19:02.637097Z","iopub.status.idle":"2023-04-20T15:19:02.691514Z","shell.execute_reply.started":"2023-04-20T15:19:02.637044Z","shell.execute_reply":"2023-04-20T15:19:02.690531Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(200, 200)"},"metadata":{}}]},{"cell_type":"code","source":"STOP_WORDS = stopwords.words('english')\n\ndef word2features(sent, i):\n    sent = sent.split(\" \")\n    word = sent[i]\n    global STOP_WORDS\n\n    features = {\n        'bias': 1.0, \n        'word.lower()': word.lower(), \n        'word[-3:]': word[-3:],\n        'word[-2:]': word[-2:],\n        'word.isupper()': word.isupper(),\n        'word.istitle()': word.istitle(),\n        'word.isdigit()': word.isdigit(),\n        #'word.length':len(word),\n        'word.stop_word':word.lower() in STOP_WORDS,\n    }\n    if i > 0:\n        word1 = sent[i-1]\n        features.update({\n            '-1:word.lower()': word1.lower(),\n            '-1:word.istitle()': word1.istitle(),\n            '-1:word.isupper()': word1.isupper(),\n            #'-1:word.length':len(word1),\n            '-1:word.stop_word':word1.lower() in STOP_WORDS\n        })\n    else:\n        features['BOS'] = True\n    if i < len(sent)-1:\n        word1 = sent[i+1]\n        features.update({\n            '+1:word.lower()': word1.lower(),\n            '+1:word.istitle()': word1.istitle(),\n            '+1:word.isupper()': word1.isupper(),\n            #'+1:word.length':len(word1),\n            '+1:word.stop_words':word1.lower() in STOP_WORDS\n        })\n    else:\n        features['EOS'] = True\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:02.694292Z","iopub.execute_input":"2023-04-20T15:19:02.694725Z","iopub.status.idle":"2023-04-20T15:19:02.709210Z","shell.execute_reply.started":"2023-04-20T15:19:02.694685Z","shell.execute_reply":"2023-04-20T15:19:02.708204Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def doword2features(sent):\n    return [word2features(sent, i) for i in range(len(sent.split()))]\n\ndef labels(sent):\n    return [label for label in sent.split()] ","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:02.710915Z","iopub.execute_input":"2023-04-20T15:19:02.711495Z","iopub.status.idle":"2023-04-20T15:19:02.718552Z","shell.execute_reply.started":"2023-04-20T15:19:02.711459Z","shell.execute_reply":"2023-04-20T15:19:02.716815Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_x = [doword2features(s) for s in train_utterance]\ntrain_y = [labels(s) for s in train_slots]\nval_x = [doword2features(s) for s in val_utterance]\nval_y = [labels(s) for s in val_slots]","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:02.719759Z","iopub.execute_input":"2023-04-20T15:19:02.720046Z","iopub.status.idle":"2023-04-20T15:19:03.869089Z","shell.execute_reply.started":"2023-04-20T15:19:02.720007Z","shell.execute_reply":"2023-04-20T15:19:03.868059Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"len(train_x), len(train_y), len(val_x), len(val_y)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:03.871062Z","iopub.execute_input":"2023-04-20T15:19:03.871473Z","iopub.status.idle":"2023-04-20T15:19:03.879733Z","shell.execute_reply.started":"2023-04-20T15:19:03.871434Z","shell.execute_reply":"2023-04-20T15:19:03.878539Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(13084, 13084, 200, 200)"},"metadata":{}}]},{"cell_type":"code","source":"crf = sklearn_crfsuite.CRF(\n    algorithm='lbfgs',\n    c1=0.1,\n    c2=0.1,\n    max_iterations=250,\n    all_possible_transitions=True\n)\n\ntry:\n    crf.fit(train_x, train_y)\nexcept AttributeError:\n    pass","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:19:03.881106Z","iopub.execute_input":"2023-04-20T15:19:03.882197Z","iopub.status.idle":"2023-04-20T15:29:07.479540Z","shell.execute_reply.started":"2023-04-20T15:19:03.882158Z","shell.execute_reply":"2023-04-20T15:29:07.478386Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"predicted = crf.predict(val_x)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:29:07.481270Z","iopub.execute_input":"2023-04-20T15:29:07.481679Z","iopub.status.idle":"2023-04-20T15:29:07.523954Z","shell.execute_reply.started":"2023-04-20T15:29:07.481640Z","shell.execute_reply":"2023-04-20T15:29:07.523037Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hackathon-online-nlu-slot-filling2/test.csv')\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:29:07.525542Z","iopub.execute_input":"2023-04-20T15:29:07.525926Z","iopub.status.idle":"2023-04-20T15:29:07.577355Z","shell.execute_reply.started":"2023-04-20T15:29:07.525890Z","shell.execute_reply":"2023-04-20T15:29:07.576207Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   id                                          utterance\n0   1   add this tune to my playlist guest list mashable\n1   2  i d like to listen to the song the natural farmer\n2   3  i need a table for 8 people at a restaurant in...\n3   4  open the canciones del recuerdo playlist and p...\n4   5  i want to hear something from the top-fifty by...\n5   6  i rate the mathematical magpie chronicle a 0 of 6\n6   7  show the movie schedule of animated movies clo...\n7   8  what time is careful  he might hear you playin...\n8   9   i m looking to get a seat at a brasserie in togo\n9  10      i need a table for 8 during midday in montana","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>utterance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>add this tune to my playlist guest list mashable</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>i d like to listen to the song the natural farmer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>i need a table for 8 people at a restaurant in...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>open the canciones del recuerdo playlist and p...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>i want to hear something from the top-fifty by...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>i rate the mathematical magpie chronicle a 0 of 6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>show the movie schedule of animated movies clo...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>what time is careful  he might hear you playin...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>i m looking to get a seat at a brasserie in togo</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>i need a table for 8 during midday in montana</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#df = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:29:07.579165Z","iopub.execute_input":"2023-04-20T15:29:07.579512Z","iopub.status.idle":"2023-04-20T15:29:07.583981Z","shell.execute_reply.started":"2023-04-20T15:29:07.579477Z","shell.execute_reply":"2023-04-20T15:29:07.582774Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"test_utterance = [doword2features(s) for s in df['utterance']]","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:29:07.585682Z","iopub.execute_input":"2023-04-20T15:29:07.586052Z","iopub.status.idle":"2023-04-20T15:29:07.624543Z","shell.execute_reply.started":"2023-04-20T15:29:07.586018Z","shell.execute_reply":"2023-04-20T15:29:07.623672Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"predicted_test = crf.predict(test_utterance)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:29:07.625903Z","iopub.execute_input":"2023-04-20T15:29:07.626258Z","iopub.status.idle":"2023-04-20T15:29:07.680976Z","shell.execute_reply.started":"2023-04-20T15:29:07.626225Z","shell.execute_reply":"2023-04-20T15:29:07.680113Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#for i in range(2544):\n    #predicted_sub.append('K')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:29:07.682409Z","iopub.execute_input":"2023-04-20T15:29:07.682712Z","iopub.status.idle":"2023-04-20T15:29:07.688202Z","shell.execute_reply.started":"2023-04-20T15:29:07.682680Z","shell.execute_reply":"2023-04-20T15:29:07.686932Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#predicted_test","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:29:07.689753Z","iopub.execute_input":"2023-04-20T15:29:07.690113Z","iopub.status.idle":"2023-04-20T15:29:07.697684Z","shell.execute_reply.started":"2023-04-20T15:29:07.690080Z","shell.execute_reply":"2023-04-20T15:29:07.696760Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import csv\n\nheader = ['id_word', 'slots']  # define the header row\n\nwith open('submission.csv', mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(header)\n    for i, row in enumerate(predicted_test):\n        for j, value in enumerate(row):\n            writer.writerow([f\"{i+1}_{j+1}\", value])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:29:07.699704Z","iopub.execute_input":"2023-04-20T15:29:07.700064Z","iopub.status.idle":"2023-04-20T15:29:07.712247Z","shell.execute_reply.started":"2023-04-20T15:29:07.700031Z","shell.execute_reply":"2023-04-20T15:29:07.711376Z"},"trusted":true},"execution_count":26,"outputs":[]}]}